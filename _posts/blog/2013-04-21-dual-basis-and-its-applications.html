---
layout: post
title: Dual basis and its applications
date: '2013-04-21T13:55:00.001-04:00'
author: Joey Dumont
tags: 
modified_time: '2013-04-21T13:55:16.566-04:00'
blogger_id: tag:blogger.com,1999:blog-3428773921792525741.post-121644364808692860
blogger_orig_url: http://blog.joey-dumont.ca/2013/04/dual-basis-and-its-applications.html
---

<div dir="ltr" style="text-align:  justify;" trbidi="on">&nbsp;&nbsp;&nbsp;In the physical sciences, the scientist mostly tries to put the problem in a form that is easily solvable. One such form is the eigenvector expansion where one solves a simpler problem and assumes that the more difficult problem can be decomposed in a sum of the simpler solutions.  </div> <div dir="ltr", style="text-align: justify;" trbidy="on">&nbsp;&nbsp;&nbsp;This kind of problem generally involves solving the eigenvalue problem of a particular matrix. Assuming that you know how to do that, this is all fine and well. However, it is also common to use orthogonality relations to simplify the solution. For a general matrix $A$, the resulting eigenvectors need not be orthogonal. That's a problem. Let's look for a solution.  </div> <div dir="ltr", style="text-align: justify;" trbidy="on">&nbsp;&nbsp;&nbsp;Adopting the braket notation and denoting an eigenvector by $|e_i\rangle$, we seek new vectors such that  $$ \langle f^i|e_j\rangle = \delta^i_j.$$ To see how we can compute those vectors, consider the original eigenvalue problem  $$ \label{eq:eigenvalue}A|e_i\rangle = \lambda_i|e_i\rangle .$$ We will use the fact that both sets of eigenvectors are complete so that there exists a closure relation  $$ \sum_i |e_i\rangle\langle f^i| = 1.$$ Now, pre-multipling \eqref{eq:eigenvalue} by $\langle f^j|$ and post-multiplying by $\langle f^i|$ and summing over the index $i$, we get  $$ \sum_i \langle f^j|A|e_i\rangle\langle f^i| = \sum_i \lambda_i \langle f^j|e_i\rangle\langle f^i|.$$ Using our orthogonality relations on the right-hand side of this last equation and the closure relation on the left-hand side, we get  $$ \langle f^j| A = \sum_i \lambda_i \langle f^i| \delta^j_i. $$ The Dirac delta makes the sum disappear as usual and we have  $$ \langle f^j| A = \lambda_j \langle f^j|. $$ In other words, the $\langle f^j|$ are the row eigenvectors of  $A$ and have the same eigenvalues as the set $|e_i\rangle$! </div> <div dir="ltr", style="text-align: justify;" trbidy="on">&nbsp;&nbsp;&nbsp;So we find that these new "contravariant" vectors are actually the <i>left eigenvectors</i> of the matrix $A$. Moreover, the decomposition of any vector in the original set of covariant vectors is given by the dot product with the vectors in the dual basis. Say we have a vector $|v\rangle$, its decomposition is given by  $$ |v\rangle = \sum_i \langle f^i|v\rangle|e_i\rangle .$$ </div> <div dir="ltr", style="text-align: justify;" trbidy="on">&nbsp;&nbsp;&nbsp;This eigendecomposition is incredibly useful in solving differential equations, smoothing numerically unstable solutions...  </div>