<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Dual basis and its applications &#8211; Study of Nature</title>
<meta name="description" content="&nbsp;&nbsp;&nbsp;In the physical sciences, the scientist mostly tries to put the problem in a form that is easily solvable. One such form is the eigenvector expansion where one solves a simpler problem and assumes that the more difficult problem can be decomposed in a sum of the simpler solutions.   &nbsp;&nbsp;&nbsp;This kind of problem generally involves solving the eigenvalue problem of a particular matrix. Assuming that you know how to do that, this is all fine and well. However, it is also common to use orthogonality relations to simplify the solution. For a general matrix $A$, the resulting eigenvectors need not be orthogonal. That's a problem. Let's look for a solution.   &nbsp;&nbsp;&nbsp;Adopting the braket notation and denoting an eigenvector by $|e_i\rangle$, we seek new vectors such that  $$ \langle f^i|e_j\rangle = \delta^i_j.$$ To see how we can compute those vectors, consider the original eigenvalue problem  $$ \label{eq:eigenvalue}A|e_i\rangle = \lambda_i|e_i\rangle .$$ We will use the fact that both sets of eigenvectors are complete so that there exists a closure relation  $$ \sum_i |e_i\rangle\langle f^i| = 1.$$ Now, pre-multipling \eqref{eq:eigenvalue} by $\langle f^j|$ and post-multiplying by $\langle f^i|$ and summing over the index $i$, we get  $$ \sum_i \langle f^j|A|e_i\rangle\langle f^i| = \sum_i \lambda_i \langle f^j|e_i\rangle\langle f^i|.$$ Using our orthogonality relations on the right-hand side of this last equation and the closure relation on the left-hand side, we get  $$ \langle f^j| A = \sum_i \lambda_i \langle f^i| \delta^j_i. $$ The Dirac delta makes the sum disappear as usual and we have  $$ \langle f^j| A = \lambda_j \langle f^j|. $$ In other words, the $\langle f^j|$ are the row eigenvectors of  $A$ and have the same eigenvalues as the set $|e_i\rangle$!  &nbsp;&nbsp;&nbsp;So we find that these new "contravariant" vectors are actually the left eigenvectors of the matrix $A$. Moreover, the decomposition of any vector in the original set of covariant vectors is given by the dot product with the vectors in the dual basis. Say we have a vector $|v\rangle$, its decomposition is given by  $$ |v\rangle = \sum_i \langle f^i|v\rangle|e_i\rangle .$$  &nbsp;&nbsp;&nbsp;This eigendecomposition is incredibly useful in solving differential equations, smoothing numerically unstable solutions...  ">
<meta name="keywords" content="">


<!-- Twitter Cards -->
<meta name="twitter:title" content="Dual basis and its applications">
<meta name="twitter:description" content="&nbsp;&nbsp;&nbsp;In the physical sciences, the scientist mostly tries to put the problem in a form that is easily solvable. One such form is the eigenvector expansion where one solves a simpler problem and assumes that the more difficult problem can be decomposed in a sum of the simpler solutions.   &nbsp;&nbsp;&nbsp;This kind of problem generally involves solving the eigenvalue problem of a particular matrix. Assuming that you know how to do that, this is all fine and well. However, it is also common to use orthogonality relations to simplify the solution. For a general matrix $A$, the resulting eigenvectors need not be orthogonal. That's a problem. Let's look for a solution.   &nbsp;&nbsp;&nbsp;Adopting the braket notation and denoting an eigenvector by $|e_i\rangle$, we seek new vectors such that  $$ \langle f^i|e_j\rangle = \delta^i_j.$$ To see how we can compute those vectors, consider the original eigenvalue problem  $$ \label{eq:eigenvalue}A|e_i\rangle = \lambda_i|e_i\rangle .$$ We will use the fact that both sets of eigenvectors are complete so that there exists a closure relation  $$ \sum_i |e_i\rangle\langle f^i| = 1.$$ Now, pre-multipling \eqref{eq:eigenvalue} by $\langle f^j|$ and post-multiplying by $\langle f^i|$ and summing over the index $i$, we get  $$ \sum_i \langle f^j|A|e_i\rangle\langle f^i| = \sum_i \lambda_i \langle f^j|e_i\rangle\langle f^i|.$$ Using our orthogonality relations on the right-hand side of this last equation and the closure relation on the left-hand side, we get  $$ \langle f^j| A = \sum_i \lambda_i \langle f^i| \delta^j_i. $$ The Dirac delta makes the sum disappear as usual and we have  $$ \langle f^j| A = \lambda_j \langle f^j|. $$ In other words, the $\langle f^j|$ are the row eigenvectors of  $A$ and have the same eigenvalues as the set $|e_i\rangle$!  &nbsp;&nbsp;&nbsp;So we find that these new "contravariant" vectors are actually the left eigenvectors of the matrix $A$. Moreover, the decomposition of any vector in the original set of covariant vectors is given by the dot product with the vectors in the dual basis. Say we have a vector $|v\rangle$, its decomposition is given by  $$ |v\rangle = \sum_i \langle f^i|v\rangle|e_i\rangle .$$  &nbsp;&nbsp;&nbsp;This eigendecomposition is incredibly useful in solving differential equations, smoothing numerically unstable solutions...  ">
<meta name="twitter:site" content="@jayd_phys">
<meta name="twitter:creator" content="@jayd_phys">

<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://valandil.github.io/images/site-logo.png">

<!-- Open Graph -->
<meta property="og:locale" content="en_CA">
<meta property="og:type" content="article">
<meta property="og:title" content="Dual basis and its applications">
<meta property="og:description" content="&nbsp;&nbsp;&nbsp;In the physical sciences, the scientist mostly tries to put the problem in a form that is easily solvable. One such form is the eigenvector expansion where one solves a simpler problem and assumes that the more difficult problem can be decomposed in a sum of the simpler solutions.   &nbsp;&nbsp;&nbsp;This kind of problem generally involves solving the eigenvalue problem of a particular matrix. Assuming that you know how to do that, this is all fine and well. However, it is also common to use orthogonality relations to simplify the solution. For a general matrix $A$, the resulting eigenvectors need not be orthogonal. That's a problem. Let's look for a solution.   &nbsp;&nbsp;&nbsp;Adopting the braket notation and denoting an eigenvector by $|e_i\rangle$, we seek new vectors such that  $$ \langle f^i|e_j\rangle = \delta^i_j.$$ To see how we can compute those vectors, consider the original eigenvalue problem  $$ \label{eq:eigenvalue}A|e_i\rangle = \lambda_i|e_i\rangle .$$ We will use the fact that both sets of eigenvectors are complete so that there exists a closure relation  $$ \sum_i |e_i\rangle\langle f^i| = 1.$$ Now, pre-multipling \eqref{eq:eigenvalue} by $\langle f^j|$ and post-multiplying by $\langle f^i|$ and summing over the index $i$, we get  $$ \sum_i \langle f^j|A|e_i\rangle\langle f^i| = \sum_i \lambda_i \langle f^j|e_i\rangle\langle f^i|.$$ Using our orthogonality relations on the right-hand side of this last equation and the closure relation on the left-hand side, we get  $$ \langle f^j| A = \sum_i \lambda_i \langle f^i| \delta^j_i. $$ The Dirac delta makes the sum disappear as usual and we have  $$ \langle f^j| A = \lambda_j \langle f^j|. $$ In other words, the $\langle f^j|$ are the row eigenvectors of  $A$ and have the same eigenvalues as the set $|e_i\rangle$!  &nbsp;&nbsp;&nbsp;So we find that these new "contravariant" vectors are actually the left eigenvectors of the matrix $A$. Moreover, the decomposition of any vector in the original set of covariant vectors is given by the dot product with the vectors in the dual basis. Say we have a vector $|v\rangle$, its decomposition is given by  $$ |v\rangle = \sum_i \langle f^i|v\rangle|e_i\rangle .$$  &nbsp;&nbsp;&nbsp;This eigendecomposition is incredibly useful in solving differential equations, smoothing numerically unstable solutions...  ">
<meta property="og:url" content="https://valandil.github.io/dual-basis-and-its-applications/">
<meta property="og:site_name" content="Study of Nature">





<link rel="canonical" href="https://valandil.github.io/dual-basis-and-its-applications/">
<link href="https://valandil.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Study of Nature Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="https://valandil.github.io/assets/css/main.css">
<!-- Webfonts -->
<script src="https://use.edgefonts.net/source-sans-pro:n2,i2,n3,i3,n4,i4,n6,i6,n7,i7,n9,i9;source-code-pro:n4,n7;volkhov.js"></script>

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
  <script src="https://valandil.github.io/assets/js/vendor/html5shiv.min.js"></script>
  <script src="https://valandil.github.io/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="https://valandil.github.io/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>


<!-- MathJax -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="https://valandil.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="https://valandil.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="https://valandil.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="https://valandil.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="https://valandil.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://valandil.github.io/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body id="post">

<div class="navigation-wrapper">
	<nav role="navigation" id="site-nav" class="animated drop">
	    <ul>
      <li><a href="https://valandil.github.io/" rel="home" title="Study of Nature"><img src="https://valandil.github.io/images/site-logo.png" width="25" height="25" alt="Study of Nature logo" class="animated fadeInDown"></a>
      
		    
		    <li><a href="https://valandil.github.io/about/" >About</a></li>
		  
		    
		    <li><a href="https://valandil.github.io/articles/" >Academic Publications</a></li>
		  
		    
		    <li><a href="https://valandil.github.io/blog/" >Blog</a></li>
		  
		    
		    <li><a href="https://valandil.github.io/search/" >Search</a></li>
		  
	    </ul>
	</nav>
</div><!-- /.navigation-wrapper -->

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->

{}


<div class="js-menu-screen menu-screen"></div>


<div id="main" role="main">
  <article class="hentry">
    
    <div class="entry-wrapper">
      <header class="entry-header">
        
          <h1 class="entry-title">Dual basis and its applications</h1>
        
        <ul class="entry-tags">
          
        </ul>
      </header>
      <footer class="entry-meta">
        
        
          <img src="https://valandil.github.io/images/jayd_photo.JPG" class="bio-photo" alt="Joey Dumont bio photo"/>
        
        <span class="author vcard">By <span class="fn">Joey Dumont</span></span>
        <span class="entry-date date published"><time datetime="2013-04-21T13:55:00-04:00"><i class="fa fa-calendar-o"></i> April 21, 2013</time></span>
        
        <span class="entry-comments"><i class="fa fa-comment-o"></i> <a href="#disqus_thread">Comment</a></span>
        
        
      </footer>
      <div class="entry-content">
        <div dir="ltr" style="text-align:  justify;" trbidi="on">&nbsp;&nbsp;&nbsp;In the physical sciences, the scientist mostly tries to put the problem in a form that is easily solvable. One such form is the eigenvector expansion where one solves a simpler problem and assumes that the more difficult problem can be decomposed in a sum of the simpler solutions.  </div> <div dir="ltr", style="text-align: justify;" trbidy="on">&nbsp;&nbsp;&nbsp;This kind of problem generally involves solving the eigenvalue problem of a particular matrix. Assuming that you know how to do that, this is all fine and well. However, it is also common to use orthogonality relations to simplify the solution. For a general matrix $A$, the resulting eigenvectors need not be orthogonal. That's a problem. Let's look for a solution.  </div> <div dir="ltr", style="text-align: justify;" trbidy="on">&nbsp;&nbsp;&nbsp;Adopting the braket notation and denoting an eigenvector by $|e_i\rangle$, we seek new vectors such that  $$ \langle f^i|e_j\rangle = \delta^i_j.$$ To see how we can compute those vectors, consider the original eigenvalue problem  $$ \label{eq:eigenvalue}A|e_i\rangle = \lambda_i|e_i\rangle .$$ We will use the fact that both sets of eigenvectors are complete so that there exists a closure relation  $$ \sum_i |e_i\rangle\langle f^i| = 1.$$ Now, pre-multipling \eqref{eq:eigenvalue} by $\langle f^j|$ and post-multiplying by $\langle f^i|$ and summing over the index $i$, we get  $$ \sum_i \langle f^j|A|e_i\rangle\langle f^i| = \sum_i \lambda_i \langle f^j|e_i\rangle\langle f^i|.$$ Using our orthogonality relations on the right-hand side of this last equation and the closure relation on the left-hand side, we get  $$ \langle f^j| A = \sum_i \lambda_i \langle f^i| \delta^j_i. $$ The Dirac delta makes the sum disappear as usual and we have  $$ \langle f^j| A = \lambda_j \langle f^j|. $$ In other words, the $\langle f^j|$ are the row eigenvectors of  $A$ and have the same eigenvalues as the set $|e_i\rangle$! </div> <div dir="ltr", style="text-align: justify;" trbidy="on">&nbsp;&nbsp;&nbsp;So we find that these new "contravariant" vectors are actually the <i>left eigenvectors</i> of the matrix $A$. Moreover, the decomposition of any vector in the original set of covariant vectors is given by the dot product with the vectors in the dual basis. Say we have a vector $|v\rangle$, its decomposition is given by  $$ |v\rangle = \sum_i \langle f^i|v\rangle|e_i\rangle .$$ </div> <div dir="ltr", style="text-align: justify;" trbidy="on">&nbsp;&nbsp;&nbsp;This eigendecomposition is incredibly useful in solving differential equations, smoothing numerically unstable solutions...  </div>
        
          <div id="disqus_thread"></div><!-- /#disqus_thread -->
          
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'studyofnature'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'https://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

        
      </div><!-- /.entry-content -->
    </div><!-- /.entry-wrapper -->
    <nav class="pagination" role="navigation">
      
        <a href="https://valandil.github.io/interfacing-armadillo-and-fftw/" class="btn" title="Interfacing Armadillo and FFTW">Previous</a>
      
      
        <a href="https://valandil.github.io/google-calendar-material-design/" class="btn" title="Google Calendar Material Design">Next</a>
      
    </nav><!-- /.pagination -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo" class="entry-wrapper">
    

<span>&copy; 2017 Joey Dumont. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using a <a href="https://github.com/valandil/" the <a href="https://mademistakes.com/work/so-simple-jekyll-theme/" rel="nofollow">So Simple Theme</a>.</span>
<div class="social-icons">
	<a href="https://twitter.com/jayd_phys" title="Joey Dumont on Twitter" target="_blank"><i class="fa fa-twitter-square fa-2x"></i></a>
	
	
	
	
	
	
	<a href="https://github.com/valandil" title="Joey Dumont on Github" target="_blank"><i class="fa fa-github-square fa-2x"></i></a>
	
  
	
  <a href="https://valandil.github.io/feed.xml" title="Atom/RSS feed"><i class="fa fa-rss-square fa-2x"></i></a>
</div><!-- /.social-icons -->

  </footer>
</div><!-- /.footer-wrapper -->

<script type="text/javascript">
  var BASE_URL = 'https://valandil.github.io';
</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="https://valandil.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="https://valandil.github.io/assets/js/scripts.min.js"></script>




</body>
</html>
